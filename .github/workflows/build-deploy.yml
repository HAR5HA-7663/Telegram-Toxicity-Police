name: build-deploy

on:
  push:
    branches: ["main"]
  workflow_dispatch: # Allow manual triggers

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: us-east-2
  ACCOUNT_ID: 898919247265
  CLUSTER_NAME: tg-moderator
  ECR_TOXICITY_REPO: toxicity-svc
  ECR_BOT_REPO: telegram-bot-svc

jobs:
  setup-infrastructure:
    runs-on: ubuntu-latest
    outputs:
      cluster-exists: ${{ steps.check-cluster.outputs.exists }}
      ecr-exists: ${{ steps.check-ecr.outputs.exists }}
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS creds (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ env.ACCOUNT_ID }}:role/GitHubOIDCDeployRole
          aws-region: ${{ env.AWS_REGION }}

      - name: Check if EKS cluster exists
        id: check-cluster
        run: |
          if aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "âœ… EKS cluster exists"
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "âŒ EKS cluster does not exist"
          fi

      - name: Check if ECR repositories exist
        id: check-ecr
        run: |
          if aws ecr describe-repositories --repository-names ${{ env.ECR_TOXICITY_REPO }} --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "âœ… ECR repositories exist"
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "âŒ ECR repositories do not exist"
          fi

      - name: Create ECR repositories if needed
        if: steps.check-ecr.outputs.exists == 'false'
        run: |
          echo "Creating ECR repositories..."
          aws ecr create-repository --repository-name ${{ env.ECR_TOXICITY_REPO }} --region ${{ env.AWS_REGION }} || true
          aws ecr create-repository --repository-name ${{ env.ECR_BOT_REPO }} --region ${{ env.AWS_REGION }} || true
          echo "âœ… ECR repositories created"

      - name: Install eksctl
        if: steps.check-cluster.outputs.exists == 'false'
        run: |
          curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_Linux_amd64.tar.gz"
          tar -xzf eksctl_Linux_amd64.tar.gz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin
          eksctl version

      - name: Create EKS cluster if needed (Minimal for demo)
        if: steps.check-cluster.outputs.exists == 'false'
        run: |
          echo "Creating minimal EKS cluster for demo..."
          eksctl create cluster \
            --name ${{ env.CLUSTER_NAME }} \
            --region ${{ env.AWS_REGION }} \
            --node-type t3.small \
            --nodes 1 \
            --nodes-min 1 \
            --nodes-max 2 \
            --managed \
            --spot
          echo "âœ… EKS cluster created"

      - name: Install metrics-server if needed
        if: steps.check-cluster.outputs.exists == 'false'
        run: |
          echo "Checking if metrics-server already exists..."
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

          # Check if metrics-server addon is already installed
          if kubectl get deployment metrics-server -n kube-system &> /dev/null; then
            echo "âœ… Metrics-server already installed as EKS addon"
          else
            echo "Installing metrics-server for HPA..."
            kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
            echo "âœ… Metrics-server installed"
          fi

      - name: Install Prometheus if needed
        continue-on-error: true # Don't fail deployment if Prometheus times out
        timeout-minutes: 8
        run: |
          # Update kubeconfig to check Prometheus status
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

          # Check if Prometheus is already installed
          if kubectl get namespace monitoring &> /dev/null && \
             helm list -n monitoring 2>/dev/null | grep -q prometheus; then
            echo "âœ… Prometheus already installed"
            exit 0
          fi

          echo "Installing Prometheus for monitoring (background installation)..."
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update

          # Install without waiting - let it complete in background
          helm install prometheus prometheus-community/kube-prometheus-stack \
            -n monitoring \
            --create-namespace \
            --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \
            --timeout 15m &

          echo "âœ… Prometheus installation started in background"
          echo "   It will be ready in 5-10 minutes"

      - name: Install AWS Load Balancer Controller if needed
        if: steps.check-cluster.outputs.exists == 'false'
        run: |
          echo "Setting up AWS Load Balancer Controller..."

          # Update kubeconfig
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

          # Download IAM policy
          curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json

          # Create IAM policy
          aws iam create-policy \
            --policy-name AWSLoadBalancerControllerIAMPolicy \
            --policy-document file://iam-policy.json 2>/dev/null || echo "Policy already exists"

          # Create service account
          eksctl create iamserviceaccount \
            --cluster=${{ env.CLUSTER_NAME }} \
            --namespace=kube-system \
            --name=aws-load-balancer-controller \
            --attach-policy-arn=arn:aws:iam::${{ env.ACCOUNT_ID }}:policy/AWSLoadBalancerControllerIAMPolicy \
            --override-existing-serviceaccounts \
            --region=${{ env.AWS_REGION }} \
            --approve

          # Install Helm
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

          # Install ALB controller
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update
          helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
            -n kube-system \
            --set clusterName=${{ env.CLUSTER_NAME }} \
            --set serviceAccount.create=false \
            --set serviceAccount.name=aws-load-balancer-controller

          echo "âœ… AWS Load Balancer Controller installed"

      - name: Grant kubectl access to IAM user
        run: |
          echo "ðŸ” Granting kubectl access to IAM user HAR5HA..."

          # Update kubeconfig
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

          # Install eksctl if not present
          if ! command -v eksctl &> /dev/null; then
            curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_Linux_amd64.tar.gz"
            tar -xzf eksctl_Linux_amd64.tar.gz -C /tmp
            sudo mv /tmp/eksctl /usr/local/bin
          fi

          # Create access entry for IAM user (EKS 1.32+ API-based auth)
          eksctl create accessentry \
            --cluster ${{ env.CLUSTER_NAME }} \
            --region ${{ env.AWS_REGION }} \
            --principal-arn arn:aws:iam::${{ env.ACCOUNT_ID }}:user/HAR5HA \
            --type STANDARD \
            --username HAR5HA 2>/dev/null || echo "Access entry already exists"

          # Associate cluster admin policy
          eksctl associate accesspolicy \
            --cluster ${{ env.CLUSTER_NAME }} \
            --region ${{ env.AWS_REGION }} \
            --principal-arn arn:aws:iam::${{ env.ACCOUNT_ID }}:user/HAR5HA \
            --policy-arn arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy \
            --access-scope type=cluster 2>/dev/null || echo "Policy already associated"

          echo "âœ… kubectl access granted to IAM user HAR5HA"

      - name: Ensure AWS Load Balancer Controller is installed
        run: |
          echo "ðŸ” Checking AWS Load Balancer Controller status..."

          # Update kubeconfig
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

          # Check if ALB controller exists
          if kubectl get deployment aws-load-balancer-controller -n kube-system &> /dev/null; then
            echo "âœ… AWS Load Balancer Controller already installed"
            kubectl get deployment aws-load-balancer-controller -n kube-system
            exit 0
          fi

          echo "ðŸ“¦ Installing AWS Load Balancer Controller..."

          # Download IAM policy
          curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json

          # Create IAM policy if doesn't exist
          aws iam create-policy \
            --policy-name AWSLoadBalancerControllerIAMPolicy \
            --policy-document file://iam-policy.json 2>/dev/null || echo "IAM policy already exists"

          # Install eksctl if not present
          if ! command -v eksctl &> /dev/null; then
            curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_Linux_amd64.tar.gz"
            tar -xzf eksctl_Linux_amd64.tar.gz -C /tmp
            sudo mv /tmp/eksctl /usr/local/bin
          fi

          # Create OIDC provider if not exists
          eksctl utils associate-iam-oidc-provider \
            --cluster ${{ env.CLUSTER_NAME }} \
            --region ${{ env.AWS_REGION }} \
            --approve 2>/dev/null || echo "OIDC provider already exists"

          # Create service account
          eksctl create iamserviceaccount \
            --cluster=${{ env.CLUSTER_NAME }} \
            --namespace=kube-system \
            --name=aws-load-balancer-controller \
            --attach-policy-arn=arn:aws:iam::${{ env.ACCOUNT_ID }}:policy/AWSLoadBalancerControllerIAMPolicy \
            --override-existing-serviceaccounts \
            --region=${{ env.AWS_REGION }} \
            --approve

          # Install Helm if not present
          if ! command -v helm &> /dev/null; then
            curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          fi

          # Install ALB controller
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update
          helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
            -n kube-system \
            --set clusterName=${{ env.CLUSTER_NAME }} \
            --set serviceAccount.create=false \
            --set serviceAccount.name=aws-load-balancer-controller

          echo "âœ… AWS Load Balancer Controller installed"

  build-and-push:
    needs: setup-infrastructure
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS creds (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ env.ACCOUNT_ID }}:role/GitHubOIDCDeployRole
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to ECR
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build & push toxicity-svc
        run: |
          IMAGE=${{ env.ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.ECR_TOXICITY_REPO }}:${{ github.sha }}
          IMAGE_LATEST=${{ env.ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.ECR_TOXICITY_REPO }}:latest

          docker build -t $IMAGE services/toxicity-svc
          docker tag $IMAGE $IMAGE_LATEST
          docker push $IMAGE
          docker push $IMAGE_LATEST

          echo "toxicity_image=$IMAGE" >> $GITHUB_ENV

      - name: Build & push telegram-bot-svc
        run: |
          IMAGE=${{ env.ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.ECR_BOT_REPO }}:${{ github.sha }}
          IMAGE_LATEST=${{ env.ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.ECR_BOT_REPO }}:latest

          docker build -t $IMAGE services/telegram-bot-svc
          docker tag $IMAGE $IMAGE_LATEST
          docker push $IMAGE
          docker push $IMAGE_LATEST

          echo "bot_image=$IMAGE" >> $GITHUB_ENV

  deploy:
    needs: build-and-push
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS creds (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ env.ACCOUNT_ID }}:role/GitHubOIDCDeployRole
          aws-region: ${{ env.AWS_REGION }}

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}
          kubectl cluster-info

      - name: Deploy to Kubernetes
        run: |
          echo "Applying Kubernetes manifests..."

          # Apply in order
          kubectl apply -f k8s/00-namespace.yaml
          kubectl apply -f k8s/01-config-secrets.yaml
          kubectl apply -f k8s/10-toxicity.yaml
          kubectl apply -f k8s/20-telegram-bot.yaml
          kubectl apply -f k8s/30-ingress.yaml
          kubectl apply -f k8s/40-autoscaling.yaml

          # Wait for ServiceMonitor CRD to be ready (retry up to 3 minutes)
          echo "Checking for ServiceMonitor CRD..."
          for i in {1..36}; do
            if kubectl get crd servicemonitors.monitoring.coreos.com &> /dev/null; then
              echo "âœ… ServiceMonitor CRD found, applying monitoring..."
              kubectl apply -f k8s/50-monitoring.yaml
              echo "âœ… Monitoring manifests applied"
              break
            else
              echo "â³ Waiting for Prometheus CRDs (attempt $i/36)..."
              sleep 5
            fi
          done

          if ! kubectl get crd servicemonitors.monitoring.coreos.com &> /dev/null; then
            echo "âš ï¸  ServiceMonitor CRD not ready yet - Prometheus still installing"
            echo "   Core services will deploy anyway. Monitoring will be available later."
            echo "   To apply monitoring manually once ready:"
            echo "   kubectl apply -f k8s/50-monitoring.yaml"
          fi

          echo "âœ… Manifests applied"

      - name: Cleanup old pods to free resources
        run: |
          echo "Cleaning up old replicasets to free node resources..."

          # Delete old/pending pods for toxicity service
          kubectl delete pods -n telegram -l app=toxicity-svc --field-selector status.phase=Pending --force --grace-period=0 || true
          kubectl delete pods -n telegram -l app=toxicity-svc --field-selector status.phase=Failed --force --grace-period=0 || true

          # Delete old/pending pods for bot service  
          kubectl delete pods -n telegram -l app=telegram-bot-svc --field-selector status.phase=Pending --force --grace-period=0 || true
          kubectl delete pods -n telegram -l app=telegram-bot-svc --field-selector status.phase=Failed --force --grace-period=0 || true

          # Clean up old replicasets (keep only last 2)
          kubectl delete replicaset -n telegram -l app=toxicity-svc \
            $(kubectl get replicaset -n telegram -l app=toxicity-svc --sort-by=.metadata.creationTimestamp -o name | head -n -2 | xargs) 2>/dev/null || true
          kubectl delete replicaset -n telegram -l app=telegram-bot-svc \
            $(kubectl get replicaset -n telegram -l app=telegram-bot-svc --sort-by=.metadata.creationTimestamp -o name | head -n -2 | xargs) 2>/dev/null || true

          echo "âœ… Cleanup complete"

          # Wait a moment for resources to be freed
          sleep 10

      - name: Update deployment images
        run: |
          echo "Updating deployments with new images..."

          # Update toxicity service
          kubectl set image deployment/toxicity-svc \
            app=${{ env.ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.ECR_TOXICITY_REPO }}:${{ github.sha }} \
            -n telegram

          # Update bot service
          kubectl set image deployment/telegram-bot-svc \
            app=${{ env.ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.ECR_BOT_REPO }}:${{ github.sha }} \
            -n telegram

          echo "âœ… Images updated"

      - name: Wait for deployments to be ready
        run: |
          echo "Waiting for deployments to rollout..."

          # Check pod status first (with error handling)
          echo "=== Initial Pod Status ==="
          kubectl get pods -n telegram -o wide || echo "No pods found yet or namespace doesn't exist"

          # Give pods a moment to be created
          sleep 5

          # Wait for toxicity service
          echo ""
          echo "Waiting for toxicity-svc..."
          if ! kubectl rollout status deployment/toxicity-svc -n telegram --timeout=8m; then
            echo "âŒ toxicity-svc failed to deploy. Diagnostics:"
            kubectl describe pods -n telegram -l app=toxicity-svc || true
            kubectl logs -n telegram -l app=toxicity-svc --tail=50 || true
            kubectl get events -n telegram --sort-by='.lastTimestamp' | tail -20
            exit 1
          fi

          # Wait for bot service
          echo ""
          echo "Waiting for telegram-bot-svc..."
          if ! kubectl rollout status deployment/telegram-bot-svc -n telegram --timeout=8m; then
            echo "âŒ telegram-bot-svc failed to deploy. Diagnostics:"
            kubectl describe pods -n telegram -l app=telegram-bot-svc || true
            kubectl logs -n telegram -l app=telegram-bot-svc --tail=50 || true
            kubectl get events -n telegram --sort-by='.lastTimestamp' | tail -20
            exit 1
          fi

          echo "âœ… Deployments ready"

      - name: Get service endpoints
        run: |
          echo "=== Deployment Status ==="
          kubectl get pods -n telegram
          kubectl get svc -n telegram
          kubectl get ingress -n telegram

          echo ""
          echo "=== Load Balancer URL ==="
          ALB_URL=$(kubectl get ingress telegram-ingress -n telegram -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "Pending...")
          if [ "$ALB_URL" != "Pending..." ]; then
            echo "Webhook URL: http://$ALB_URL/webhook"
            echo "Set your Telegram webhook to: http://$ALB_URL/webhook"
          else
            echo "Load balancer is being provisioned. Check back in 3-5 minutes:"
            echo "  kubectl get ingress telegram-ingress -n telegram"
          fi

      - name: Deployment summary
        if: always()
        run: |
          echo "=== Deployment Summary ==="
          echo "Cluster: ${{ env.CLUSTER_NAME }}"
          echo "Region: ${{ env.AWS_REGION }}"
          echo "Commit: ${{ github.sha }}"
          echo ""
          echo "Images:"
          echo "  toxicity-svc: ${{ env.ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.ECR_TOXICITY_REPO }}:${{ github.sha }}"
          echo "  telegram-bot-svc: ${{ env.ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.ECR_BOT_REPO }}:${{ github.sha }}"
